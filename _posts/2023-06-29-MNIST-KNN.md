---
layout: single
title: "MNIST KNN(without library, with library)"
categories: python
tag: [ML]
toc: true
toc_sticky: true
author_profile: true
search: true
---

# 1. MNIST란?
MNIST(Modified National Institute of Standards and Technology) 데이터베이스는 다양한 이미지 처리 시스템을 교육하는 데 일반적으로 사용되는 손으로 쓴 숫자의 대규모 데이터베이스 입니다. 
본 포스트에서 사용된 데이터셋은 하나의 이미지 당 1783의 픽셀로 피처가 구성되어있습니다. 
<br><br><br><br><br>

<img src="https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png">
<br><br><br><br><br>

# 2. KNN이란?
KNN 또는 k-NN이라고도 하는 k-최근접 이웃 알고리즘은 근접성을 사용하여 개별 데이터 포인트의 그룹화에 대한 분류 또는 예측을 수행하는 비모수 지도 학습 분류기입니다. 
회귀 또는 분류 문제에 사용할 수 있지만 일반적으로 분류 알고리즘으로 사용되며 유사한 점이 서로 가까이에서 발견될 수 있다는 가정을 수행합니다.
분류 문제의 경우 다수결을 기준으로 클래스 레이블이 할당됩니다. 즉, 주어진 데이터 포인트 주변에서 가장 자주 나타나는 레이블이 사용됩니다.
<img src="https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/cdp/cf/ul/g/ef/3a/KNN.component.complex-narrative-xl-retina.ts=1653407890466.png/content/adobe-cms/us/en/topics/knn/jcr:content/root/table_of_contents/intro/complex_narrative/items/content_group/image">

# 3. MNIST-KNN
본 포스트에서는 MNIST 데이터셋을 가지고 KNN 알고리즘을 적용합니다. KNN 라이브러리를 사용하지 않는 버전과 사용하는 버전의 코드를 소개합니다.

## 3.1 KNN with library

### 3.1.1 Code
```python
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


def main():
    train = pd.read_csv("data/MNIST_train.csv")
    test = pd.read_csv("data/MNIST_test.csv")

    X = train.drop(["label"], axis=1)
    y = train["label"]

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

    model = KNeighborsClassifier(n_neighbors=3)
    model.fit(X_train, y_train)

    train_pred = model.predict(X_val)
    train_acc = accuracy_score(train_pred, y_val)
    print(f"Train-ACC = {train_acc * 100:.2f}%")

    test_pred = model.predict(test.drop(["label"], axis=1))
    test_acc = accuracy_score(test_pred, test["label"])
    print(f"Test-ACC = {test_acc * 100:.2f}%")


if __name__ == "__main__":
    main()
```

## 3.2 KNN without library

### 3.2.1 Code
```python
import numpy as np
import pandas as pd
from scipy.spatial.distance import euclidean
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


class KNN:
    def __init__(self, k=3):
        self.k = k
        self.X_train = None
        self.y_train = None

    def train(self, X, y):
        self.X_train = X
        self.y_train = y

    def get_distance(self, X_val):
        distances = []
        for x in X_val:
            dist = [euclidean(x, x_train) for x_train in self.X_train]
            distances.append(dist)

        return np.array(distances)

    def predict(self, X_val):
        distances = self.get_distance(X_val)
        y_pred = []
        for dist in distances:
            sorted_indices = np.argsort(dist)[:self.k]
            k_nearest_labels = self.y_train.iloc[sorted_indices]
            unique, counts = np.unique(k_nearest_labels, return_counts=True)
            y_pred.append(unique[np.argmax(counts)])

        return np.array(y_pred)


def main():
    train = pd.read_csv("data/MNIST_train.csv")
    test = pd.read_csv("data/MNIST_test.csv")

    X = train.drop(["label"], axis=1)
    y = train["label"]

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

    model = KNN(k=3)
    model.train(X_train.values, y_train)

    train_pred = model.predict(X_val.values)
    train_acc = accuracy_score(train_pred, y_val)
    print(f"Train-ACC = {train_acc * 100:.2f}%")

    test_pred = model.predict(test.drop(["label"], axis=1).values)
    test_acc = accuracy_score(test_pred, test["label"])
    print(f"Test-ACC = {test_acc * 100:.2f}%")


if __name__ == "__main__":
    main()
```

### 3.2.2 Code Description

#### definition
- <strong>__init__(self, k=3)</strong>: KNN 클래스의 생성자입니다. k 매개변수는 이웃의 개수를 지정하는 값으로, 기본값은 3입니다. X_train과 y_train은 None으로 초기화됩니다.
- <strong>train(self, X, y)</strong>: 모델을 훈련하는 메서드입니다. X와 y는 훈련 데이터의 특징과 레이블을 나타내는 배열 또는 데이터프레임입니다. 이 메서드를 호출하여 모델을 훈련시키면, 주어진 훈련 데이터로부터 X_train과 y_train이 설정됩니다.
- <strong>get_distance(self, X_val)</strong>: 주어진 검증 데이터(X_val)와 훈련 데이터(X_train) 사이의 거리를 계산하는 메서드입니다. 이 코드에서는 유클리디안 거리(euclidean distance)를 사용하여 거리를 계산합니다. 거리 계산 결과는 거리 행렬(distances)로 반환됩니다.
- <strong>predict(self, X_val)</strong>: 주어진 검증 데이터(X_val)에 대한 예측을 수행하는 메서드입니다. get_distance 메서드를 사용하여 검증 데이터와 훈련 데이터 간의 거리를 계산하고, 가장 가까운 k개 이웃을 선택합니다. 이웃들의 레이블을 확인하여 가장 많은 빈도를 가진 레이블을 예측값으로 선택합니다. 예측된 레이블은 y_pred 리스트에 추가되고, 최종적으로 numpy 배열로 반환됩니다.

#### predict(self, X_val) detail
- <strong>distances = self.get_distance(X_val)</strong>: get_distance 메서드를 호출하여 검증 데이터 X_val과 훈련 데이터 X_train 사이의 거리를 계산합니다. 이 거리 행렬은 distances 변수에 저장됩니다.
- <strong>sorted_indices = np.argsort(dist)[:self.k]</strong>: 거리 벡터를 오름차순으로 정렬한 후, 가장 가까운 k개 이웃의 인덱스를 선택합니다. argsort 함수는 정렬된 인덱스 배열을 반환합니다.
- <strong>k_nearest_labels = self.y_train.iloc[sorted_indices]</strong>: 선택된 인덱스를 사용하여 훈련 데이터의 레이블(y_train)에서 가장 가까운 이웃들의 레이블을 가져옵니다. iloc을 사용하여 인덱스에 해당하는 행을 선택합니다.
- <strong>unique, counts = np.unique(k_nearest_labels, return_counts=True)</strong>: 가장 가까운 이웃들의 레이블에서 고유한 값과 각 값의 빈도를 계산합니다. np.unique 함수의 return_counts 매개변수를 True로 설정하여 빈도를 반환하도록 합니다.
- <strong>y_pred.append(unique[np.argmax(counts)])</strong>: 가장 많은 빈도를 가진 레이블을 예측값으로 선택하여 y_pred 리스트에 추가합니다. np.argmax 함수를 사용하여 빈도 배열에서 최댓값의 인덱스를 가져온 후, 이를 사용하여 고유한 레이블 배열에서 예측값을 선택합니다.
- <strong>np.array(y_pred)</strong>: 최종적으로 예측된 레이블들이 담긴 y_pred 리스트를 numpy 배열로 변환하여 반환합니다.
