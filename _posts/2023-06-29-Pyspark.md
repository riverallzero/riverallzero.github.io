---
layout: single
title: "Apache Spark-PySpark(Iris Dataset)"
categories: python
tag: [spark, ML]
toc: true
toc_sticky: true
author_profile: true
search: true
---

# 1. Spark란?
Apache Spark는 SQL, 스트리밍, 머신러닝 및 그래프 처리를 위한 기본 제공 모듈이 있는 대규모 데이터 처리용 통합 분석 엔진입니다. Spark는 클라우드의 Apache Hadoop, Apache Mesos, Kubernetes에서 자체적으로 실행될 수 있으며 다양한 데이터 소스에 대해 실행될 수 있습니다.

## 1.1 Spark 설치 방법
```
$ brew install apache-spark
```

```
$ pyspark

>> Python 3.10.9 (main, Mar  1 2023, 12:20:14) [Clang 14.0.6 ] on darwin
Type "help", "copyright", "credits" or "license" for more information.
23/06/29 15:36:25 WARN Utils: Your hostname, gangdayeong-ui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 10.90.75.63 instead (on interface en0)
23/06/29 15:36:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/06/29 15:36:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.4.1
      /_/

Using Python version 3.10.9 (main, Mar  1 2023 12:20:14)
Spark context Web UI available at http://10.90.75.63:4040
Spark context available as 'sc' (master = local[*], app id = local-1688078186007).
SparkSession available as 'spark'.
```

## 1.2 Java 설치 방법
```
1. 업데이트하기
  $ brew update

2. adoptopenjdk/openjdk 추가하기
  $ brew tap adoptopenjdk/openjdk

3. 설치 가능한 모든 JDK 찾기 
  $ brew search jdk

4. 원하는 버전 설치하기(ex. 11)
  $ brew install --cask adoptopenjdk11

5. 설치 확인하기(경로)
  $ /usr/libexec/java_home -V

6. 설치 확인하기(버전)
  $ java --version
```

## 1.3 IP 주소 설정(선택사항)
```
1. 파일 접근
    $ sudo nano /etc/hosts
2. IP주소    호스트이름.local 설정
    10.10.48.195    gangdayeong-ui-MacBookAir.local
3. 호스트 파일 저장
    Control + O
4. Enter
5. 파일 닫기
    Control + X
```

# 2. PySpark란?
PySpark는 Apache Spark용 Python API입니다. Python을 사용하여 분산 환경에서 실시간 대규모 데이터 처리를 수행할 수 있습니다. 또한 데이터를 대화식으로 분석하기 위한 PySpark 셸을 제공합니다.
Python의 학습 용이성과 사용 용이성을 Apache Spark의 성능과 결합하여 Python에 익숙한 모든 사람이 모든 크기의 데이터를 처리하고 분석할 수 있도록 합니다.
Spark SQL, DataFrames, Structured Streaming, Machine Learning(MLlib) 및 Spark Core와 같은 Spark의 모든 기능을 지원합니다.

## 2.1 Iris Dataset with PySpark

```python
from pyspark.sql import SparkSession
import pandas as pd


spark = SparkSession.builder.appName("IrisPrediction").getOrCreate()

iris_df = pd.read_csv("data/iris.csv")
spark_df = spark.createDataFrame(iris_df)

# DataFrame의 첫 5개 행 출력
print(spark_df.show(5))

# 데이터의 특성 및 스키마 확인
print(spark_df.printSchema())
```

```
+---+---+---+---+-----------+
|5.1|3.5|1.4|0.2|Iris-setosa|
+---+---+---+---+-----------+
|4.9|3.0|1.4|0.2|Iris-setosa|
|4.7|3.2|1.3|0.2|Iris-setosa|
|4.6|3.1|1.5|0.2|Iris-setosa|
|5.0|3.6|1.4|0.2|Iris-setosa|
|5.4|3.9|1.7|0.4|Iris-setosa|
+---+---+---+---+-----------+
only showing top 5 rows

None
root
 |-- 5.1: double (nullable = true)
 |-- 3.5: double (nullable = true)
 |-- 1.4: double (nullable = true)
 |-- 0.2: double (nullable = true)
 |-- Iris-setosa: string (nullable = true)

None
```
