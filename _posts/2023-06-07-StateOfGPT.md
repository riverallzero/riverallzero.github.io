---
layout: post
title: 2023 Microsoft Build-State of GPT
date: 2023-06-07
splash_img_source: /assets/img/post_gpt.png
splash_img_caption: Representative Image
---

## Summarize : 2023 Microsoft Build-State of GPT
- Session: State of GPT [[Go](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)]
- Speaker: Andrej Karpathy. OpenAI
- Date: 2023.05.24

### GPT Assistant training pipeline
본 세션은 GPT assistant의 훈련 방법과 이 assistant를 애플리케이션에 효과적으로 사용할 수 있는 방법을 살펴보는 두 부분으로 나누어집니다. Assistant를 훈련하는 지금까지의 방법은 사전 훈련, 감독에 의한 미세 조정, 보상 모델링, 강화 학습의 네 가지 주요 단계이며, 이들은 순차적으로 이어지고 각 단계를 지원하는 데이터 세트, 신경망 알고리즘, 결과인 모델을 가지고 있습니다. 사전 훈련 단계에서 기본적으로 모든 계산 직업이 이루어져 훈련 연산 시간의 99%를 차지하고, 수천개의 GPU로 수개월의 훈련이 필요합니다. 이 다음 세가지 단계는 이를 더 미세하게 조정하는 것 입니다.

### Data collection
대량의 데이터는 GitHub, Wikipedia, 도서, 아카이브, 증권 거래소 등에서 가져온 데이터를 한데에 섞은 다음 정해진 비율에 따라 샘플링하여 훈련 세트를 구성합니다. 학습 전 `토큰화`라는 전처리 단계를 거치는데, 토큰화는 인터넷에서 스크랩한 원시 텍스트를 정수의 시퀀스로 변환하는 작업입니다. 이는 GPT가 작동하는 기본 표현입니다.

### Two example models
#### 어휘(vocabulary) 크기
어휘 크기는 모델이 인식할 수 있는 고유한 `토큰의 수` 입니다. 이 값은 모델이 텍스트에서 단어나 문자 등의 토큰을 인식하고 다음 토큰을 예측하는 데에 영향을 줍니다. 어휘 크기가 크면 모델은 더 다양한 텍스트 패턴을 학습할 수 있습니다.
#### 문맥(context) 길이
문맥 길이는 모델이 고려하는 `입력 시퀀스의 길이` 입니다. 이 값은 모델이 이전 토큰들을 살펴보고 다음 토큰을 예측하는 데에 사용되는 정보의 양을 결정합니다. 길이가 길수록 모델은 더 긴 의존 관계를 학습할 수 있지만 계산 비용이 증가할 수 있습니다.

### Base models learn powerful, general representations
이는 “기본 모델은 강력하고 일반적인 표현을 학습한다”라는 말로, 기본 모델은 언어 모델링 과정에서 매우 일반적인 표현을 학습하는데 이를 잘 활용하면 다양한 작업에 효과적으로 적용할 수 있다는 의미입니다. 예를 들어, 감성을 분류할 때 긍정적인 예와 부정적인 예를 모아 자연어처리 모델을 훈련시켰더라면, 감성 분류를 무시하고 대형 언어 모델의 사전 훈련을 수행한 뒤 원하는 작업에 맞게 모델을 세밀하게 조정하는 것으로 새로운 접근 방식을 사용합니다. 이 기본 모델을 GPT Assistant로 만들기 위해 `지도 학습된 세부 조정(supervised fine-tuning)`을 사용합니다. 작은 크기의 고품질 데이터셋을 수집하고, 인간에게 프롬프트와 이상적인 응답 형식의 데이터를 수집하도록 요청하고 언어 모델링을 수행해 assistant로 사용할 수 있는 모델을 얻어내는 것 입니다. 이후에는 `보상 모델링(Reward Modeling)`과 `강화학습(Reinforcement Learning)`으로 구성된 `인간 피드백을 통한 강화학습(Reinforcement Learning Human Feedback)`을 거칩니다.

### RM(Reward Modeling) & RL(Reinforcement Learning)
RLHF 파이프라인은 보상 모델링과 강화 학습 단계로 구성되며 보상 모델을 훈련한 후에는 강화 학습을 통해 모델을 개선합니다. 이러한 접근 방식은 비교와 판단을 활용하여 모델의 품질을 개선하는 데 도움이 되며 SFT 모델보다 성능이 더 우수합니다. 아래는 각 단계에 대한 과정입니다.
#### 보상모델링(RM)
-	데이터 수집 방식을 비교 형식으로 변경합니다.
-	기존에 훈련된 지도 학습된 세부 조정 모델(SFT: Supervised Fine-Tuning Model)을 사용하여 여러 가지 완성본을 생성합니다.
-	사람들이 이러한 완성본을 `순위` 매기도록 요청합니다.
-	각 완성본들은 이진 분류 형식으로 비교하여 순위를 결정합니다.
-	이를 위해 프롬프트를 행으로 나열하고, 각 행의 완성본을 다르게 만듭니다.
-	보상 모델링을 위해 특별한 보상 읽기 토큰을 추가하고, 모델은 이 토큰을 기준으로 보상을 예측합니다.
-	모델의 보상 예측과 실제 순위를 일치시키는 손실 함수를 정의하여 모델을 훈련합니다.
-	보상 모델을 훈련하면 완성본의 품질을 평가할 수 있습니다.
#### 강화학습(RL)
-	보상 모델이 준비되면 강화 학습을 수행합니다.
-	다시 한번 큰 양의 프롬프트 데이터를 사용하여 보상 모델에 대해 강화학습을 수행합니다.
-	각 프롬프트를 행으로 나열하고 SFT모델에 의해 `가중치`를 주어 학습합니다.
-	보상 모델을 통해 완성본의 품질을 평가합니다.
-	언어 모델링 손실 함수를 적용하되, 보상 모델에 의해 가중치를 주어 학습합니다.
-	학습한 후에는 완성본이 보상 모델에 의해 높은 점수를 받도록 모델이 생성합니다.
이렇게 RLHF 파이프라인을 통해 모델을 훈련하고, 배포 가능한 모델을 얻게 됩니다.

### Mode collapse
RLHF모델이 기본 모델보다 모든 측면에서 뛰어난 것은 아닙니다. 특정 상황이나 목표에 따라 추가적인 장점을 가질 수 있지만, `엔트로피(어떤 확률 분포의 불확실성이나 예측 가능성의 정도)의 손실`과 같은 단점이 있을 수 있습니다. 기본 모델은 다양한 출력을 생성할 수 있는 높은 엔트로피를 가지고 있어서 다양성이 있는 결과를 제공할 수 있습니다. 그러나 RLHF 모델은 엔트로피의 감소로 인해 더 `극단적인 결과`를 제공할 수 있고, `변동성이 낮은(유사한 결과를 반복하는) 샘플`을 출력할 수도 있습니다. 따라서 GPT Assistant 모델을 사용할 때에는 작업에 맞는 적절한 프롬프트를 사용하고, 작업을 토큰에 골고루 분산시키는 등의 접근 방식을 사용할 수 있습니다.

### Ensemble multiple attempts
이 접근 방식은 self-consistency라고도 불립니다. 예를 들어 글을 쓸 때 한번만 시도하는 것이 아니라 `여러 번` 시도하여 가장 좋은 결과를 선택하는 방법입니다. Transformer 모델은 다음 토큰을 예측할 때 좋지 않은 결과를 보일 수 있기에 여러 번 시도하고 좋은 결과를 찾는 방법을 제공한다면 문제를 더 잘 해결할 수 있습니다.

### Ask for reflection
언어 모델(LLMs)의 실수를 바로잡기 위해 사용자가 프롬프트로 확인을 요청해야만 수정할 수 있습니다. 또한 LLMs는 낮은 품질과 높은 품질의 솔루션을 구분할 수 없으며 기본적으로 이 모든 것을 모방하려고 합니다. 이는  좋은 성능을 요구하는 프롬프트를 통해 개선할 수 있으며 올바른 답을 얻기 위한 조건을 추가하는 것이 중요합니다.

### Tool use / Plugins
LLMs에게 도구와 플러그인을 제공해 문제 해결을 돕고, 검색 보강 모델을 활용하고, 제약 조건 프롬프팅을 사용하고, fine-tuning을 고려하는 등의 다양한 기법을 사용하여 성능을 향상시킬 수 있습니다.

### Default recommendations
작업을 최상의 성능을 달성하는 것과 성능을 최적화하는 것으로 나누어 진행하라고 권장합니다. 현재까지 가장 능력이 뛰어난 모델은 GPT-4이며, 자세한 프롬프트를 사용해 최상의 성능을 달성할 수 있습니다. 비용을 최적화하기 위해서는 더 낮은 용량의 모델이나 더 짧은 프롬프트를 사용하는 등의 탐색을 할 수 있습니다.
